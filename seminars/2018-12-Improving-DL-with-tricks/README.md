# Improving deep learning models with bag of tricks

This seminar contains various methods which can be 
easily add to existing models without significant 
implementation effort. All methods can possibly
increase final accuracy, generalization or adversarial 
robustness.

The seminar convers following topics:

* Cyclical Learning Rates and other schedulers,
* Training with extremely large mini-batches,
* AdamW i.e. a difference between L2 and Weight Decay,
* Label smoothing,
* Mixup,
* ZeroInit,
* Shake-Shake,
* etc ...



# Similar references:
* [Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/pdf/1812.01187.pdf)
