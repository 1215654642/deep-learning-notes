{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnf import CNF\n",
    "from neural_ode import NeuralODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 512\n",
    "\n",
    "p0 = tf.distributions.Normal(loc=[1.0, 0.0], scale=[1.0, 1.0])\n",
    "logdet0 = tf.zeros([num_samples, 1])\n",
    "x0 = tf.random_normal([num_samples, 2])\n",
    "h0 = tf.concat([x0, logdet0], axis=1)\n",
    "\n",
    "cnf_net = CNF(input_dim=2, hidden_dim=32, n_ensemble=16)\n",
    "ode = NeuralODE(model=cnf_net, t=np.linspace(1, 0.0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_and_update():\n",
    "    hN = ode.forward(inputs=h0)\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(hN)\n",
    "        xN, logdetN = hN[:, :2], hN[:, 2]\n",
    "        # L = log(p(zN))\n",
    "        mle = tf.reduce_sum(p0.log_prob(xN), -1)\n",
    "        # loss to minimize\n",
    "        loss = -tf.reduce_mean(mle - logdetN)\n",
    "\n",
    "    dloss = g.gradient(loss, hN)\n",
    "    h0_rec, dLdh0, dLdW = ode.backward(hN, dloss)          \n",
    "    optimizer.apply_gradients(zip(dLdW, cnf_net.weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "compute_gradients_and_update = tfe.defun(compute_gradients_and_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "for step in tqdm(range(1000)):\n",
    "    loss = compute_gradients_and_update()\n",
    "    loss_history.append(loss.numpy())\n",
    "    if step % 200 == 0:\n",
    "        plt.plot(loss_history)\n",
    "        plt.show()\n",
    "        hN = ode.forward(h0)\n",
    "        xN, logdetN = hN[:, :2], hN[:, 2]\n",
    "        plt.scatter(*xN.numpy().T, color='k', alpha=0.5)\n",
    "        plt.scatter(*x0.numpy().T, color='r', alpha=0.5)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hN = ode.forward(h0)\n",
    "xN, logdetN = hN[:, :2], hN[:, 2]\n",
    "plt.scatter(*xN.numpy().T, color='k', alpha=0.5)\n",
    "plt.scatter(*x0.numpy().T, color='r', alpha=0.5)\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "p0 = tf.distributions.Normal(loc=[0.0, 0.0], scale=[1.0, 1.0])\n",
    "x0 = tf.to_float(make_moons(n_samples=256, noise=0.05)[0])\n",
    "logdet0 = tf.zeros([256, 1])\n",
    "h0 = tf.concat([x0, logdet0], axis=1)\n",
    "\n",
    "cnf_net = CNF(input_dim=2, hidden_dim=32, n_ensemble=16)\n",
    "ode = NeuralODE(model=cnf_net, t=np.linspace(1, 0.0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_and_update():\n",
    "    hN = ode.forward(inputs=h0)\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(hN)\n",
    "        xN, logdetN = hN[:, :2], hN[:, 2]\n",
    "        # L = log(p(zN))\n",
    "        mle = tf.reduce_sum(p0.log_prob(xN), -1)\n",
    "        # loss to minimize\n",
    "        loss = -tf.reduce_mean(mle - logdetN)\n",
    "\n",
    "    dloss = g.gradient(loss, hN)\n",
    "    h0_rec, dLdh0, dLdW = ode.backward(hN, dloss)          \n",
    "    optimizer.apply_gradients(zip(dLdW, cnf_net.weights))\n",
    "    return loss\n",
    "\n",
    "compute_gradients_and_update = tfe.defun(compute_gradients_and_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "for step in tqdm(range(1100)):\n",
    "    loss = compute_gradients_and_update()\n",
    "    loss_history.append(loss.numpy())\n",
    "    if step % 200 == 0:\n",
    "        plt.subplot(121)\n",
    "        plt.plot(loss_history)\n",
    "        plt.subplot(122)\n",
    "        hN = ode.forward(h0)\n",
    "        xN, logdetN = hN[:, :2], hN[:, 2]\n",
    "        plt.scatter(*xN.numpy().T, color='k', alpha=0.5)\n",
    "        plt.scatter(*x0.numpy().T, color='r', alpha=0.5)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hN = ode.forward(h0)\n",
    "xN, logdetN = hN[:, :2], hN[:, 2]\n",
    "plt.scatter(*xN.numpy().T, color='k', alpha=0.5)\n",
    "plt.scatter(*x0.numpy().T, color='r', alpha=0.5)\n",
    "_ = plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode = NeuralODE(model=cnf_net, t=np.linspace(0, 1.0, 20))\n",
    "h0_reconstruction = ode.forward(inputs=hN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_rec = h0_reconstruction[:, :2]\n",
    "plt.scatter(*x0_rec.numpy().T, color='k', alpha=0.5)\n",
    "plt.scatter(*x0.numpy().T, color='r', alpha=0.5)\n",
    "_ = plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hN_sample = tf.concat([p0.sample(256), logdet0], axis=1)\n",
    "h0_reconstruction = ode.forward(inputs=hN_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_rec = h0_reconstruction[:, :2]\n",
    "plt.scatter(*x0_rec.numpy().T, color='k', alpha=0.5)\n",
    "plt.scatter(*x0.numpy().T, color='r', alpha=0.5)\n",
    "_ = plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 512\n",
    "\n",
    "logdet0 = tf.zeros([num_samples, 1])\n",
    "x0 = tf.random_normal([num_samples, 2])\n",
    "h0 = tf.concat([x0, logdet0], axis=1)\n",
    "\n",
    "cnf_net = CNF(input_dim=2, hidden_dim=32, n_ensemble=16)\n",
    "ode = NeuralODE(model=cnf_net, t=np.linspace(1, 0.0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w1(z):\n",
    "    return tf.sin(2.*np.pi*z[0]/4.)\n",
    "def w2(z):\n",
    "    return 3.*tf.exp(-.5*(((z[0]-1.)/.6))**2)\n",
    "def w3(z):\n",
    "    return 3.*(1+tf.exp(-(z[0]-1.)/.3))**-1\n",
    "\n",
    "\n",
    "def potential_energy(z):\n",
    "    z = tf.transpose(z)\n",
    "    return .5*((tf.norm(z, ord=2, axis=0) - 2.)/.4)**2 \\\n",
    "        - tf.log(tf.exp(-.5*((z[0]-2.)/.6)**2) + tf.exp(-.5*((z[0]+2.)/.6)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_and_update():\n",
    "    hN = ode.forward(inputs=h0)\n",
    "    with tf.GradientTape() as g:\n",
    "        g.watch(hN)\n",
    "        xN, logdetN = hN[:, :2], hN[:, 2]\n",
    "        # L = log(p(zN))\n",
    "        mle = - tf.reduce_sum(potential_energy(xN), -1)\n",
    "        # loss to minimize\n",
    "        loss = -tf.reduce_mean(mle - logdetN)\n",
    "\n",
    "    dloss = g.gradient(loss, hN)\n",
    "    h0_rec, dLdh0, dLdW = ode.backward(hN, dloss)          \n",
    "    optimizer.apply_gradients(zip(dLdW, cnf_net.weights))\n",
    "    return loss\n",
    "\n",
    "compute_gradients_and_update = tfe.defun(compute_gradients_and_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in tqdm(range(3001)):\n",
    "    loss = compute_gradients_and_update()\n",
    "    loss_history.append(loss.numpy())\n",
    "    if step % 200 == 0:\n",
    "        plt.subplot(121)\n",
    "        plt.plot(loss_history)\n",
    "        plt.subplot(122)\n",
    "        hN = ode.forward(h0)\n",
    "        xN, logdetN = hN[:, :2], hN[:, 2]\n",
    "        plt.scatter(*xN.numpy().T, color='k', alpha=0.5)\n",
    "        plt.scatter(*x0.numpy().T, color='r', alpha=0.5)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hN = ode.forward(h0)\n",
    "xN, logdetN = hN[:, :2], hN[:, 2]\n",
    "plt.scatter(*xN.numpy().T, color='k', alpha=0.5)\n",
    "plt.scatter(*x0.numpy().T, color='r', alpha=0.5)\n",
    "_ = plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-tf1.12",
   "language": "python",
   "name": "vision-tf1.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
