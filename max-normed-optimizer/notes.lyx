#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Gradient descent
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{\boldsymbol{\theta}}_{k+1}=\boldsymbol{\theta}_{k}-\lambda_{k}\frac{\partial\mathcal{L}\left(\boldsymbol{\theta}_{k},\boldsymbol{X}_{k}\right)}{\partial\boldsymbol{\theta}}=\boldsymbol{\theta}_{k}-\lambda_{k}\mathbf{G}_{k}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\boldsymbol{\theta}_{k}$
\end_inset

 is the parameter at k-th iteration, 
\begin_inset Formula $\boldsymbol{X}_{k}$
\end_inset

 - batch data (labels and targets) at k-th step, 
\begin_inset Formula $\lambda_{k}$
\end_inset

 gradient descent step, 
\begin_inset Formula $\mathcal{L}$
\end_inset

 - loss function.
\end_layout

\begin_layout Section
\begin_inset Formula $L_{\mathrm{\infty}}$
\end_inset

 normalized gradient descent
\end_layout

\begin_layout Standard
\begin_inset Formula $L_{\infty}$
\end_inset

 norm of the vector
\begin_inset Formula 
\[
\left\Vert \mathbf{G}_{k}\right\Vert _{\infty}=\max\left|\mathbf{G}_{k}\right|
\]

\end_inset


\begin_inset Formula $L_{\infty}$
\end_inset

 normalized gradient descent is defined as 
\begin_inset Formula 
\[
\mathbf{\boldsymbol{\theta}}_{k+1}=\boldsymbol{\theta}_{k}-\lambda_{k}\frac{\mathbf{G}_{k}}{\left\Vert \mathbf{G}_{k}\right\Vert _{\infty}+\varepsilon}.
\]

\end_inset


\end_layout

\begin_layout Section
Online iterative steepest gradient descent
\end_layout

\begin_layout Standard
Steepest gradient descent look for solution of following problem:
\begin_inset Formula 
\[
\lambda_{k}=\underset{\lambda}{\mathrm{argmin}}\mathcal{L}\left(\boldsymbol{\theta}_{k}-\lambda\frac{\mathbf{G}_{k}}{\left\Vert \mathbf{G}_{k}\right\Vert _{\infty}+\varepsilon},\boldsymbol{X}_{k}\right),
\]

\end_inset


\end_layout

\begin_layout Standard
Let
\begin_inset Formula 
\[
\widehat{\mathbf{G}}_{k}=\frac{\mathbf{G}_{k}}{\left\Vert \mathbf{G}_{k}\right\Vert _{\infty}+\varepsilon},
\]

\end_inset


\end_layout

\begin_layout Standard
Expression 
\begin_inset Formula $\boldsymbol{\theta}_{k}-\lambda\widehat{\mathbf{G}}_{k}$
\end_inset

 represents state of the parameters in the next step, hence we use batch
 data from the next step:
\begin_inset Formula 
\[
\lambda_{k}=\underset{\lambda}{\mathrm{argmin}}\mathcal{L}\left(\boldsymbol{\theta}_{k}-\lambda\widehat{\mathbf{G}}_{k},\boldsymbol{X}_{k+1}\right),
\]

\end_inset


\end_layout

\begin_layout Standard
and apply gradient descent on expression above to find best 
\begin_inset Formula $\lambda$
\end_inset

, we have 
\begin_inset Formula $\boldsymbol{\theta}_{k+1}(\lambda)=\boldsymbol{\theta}_{k}-\lambda\widehat{\mathbf{G}}_{k}$
\end_inset

, then
\begin_inset Formula 
\begin{align*}
\frac{\partial\mathcal{L}\left(\boldsymbol{\theta}_{k+1}(\lambda),\boldsymbol{X}_{k+1}\right)}{\partial\lambda} & =\frac{\partial\mathcal{L}\left(\boldsymbol{\theta}_{k+1}(\lambda),\boldsymbol{X}_{k+1}\right)^{T}}{\partial\boldsymbol{\theta}}\frac{\partial\left(\boldsymbol{\theta}_{k}-\lambda\widehat{\mathbf{G}}_{k}\right)}{\partial\lambda}\\
 & =-\frac{\partial\mathcal{L}\left(\boldsymbol{\theta}_{k+1}(\lambda),\boldsymbol{X}_{k+1}\right)^{T}}{\partial\boldsymbol{\theta}}\widehat{\mathbf{G}}_{k}=-\mathbf{G}_{k+1}^{T}\widehat{\mathbf{G}}_{k}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Hence the gradient descent for the 
\begin_inset Formula $\lambda$
\end_inset

 parameter is:
\begin_inset Formula 
\[
\lambda_{k+1}=\lambda_{k}-\beta\left(-\mathbf{G}_{k+1}^{T}\widehat{\mathbf{G}}_{k}\right)=\lambda_{k}+\beta\mathbf{G}_{k}^{T}\widehat{\mathbf{G}}_{k-1}.
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\beta$
\end_inset

 is an update step for 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Subsection
Potential problems:
\end_layout

\begin_layout Standard
First of all, from the above one can see that 
\begin_inset Formula $\lambda$
\end_inset

 can be smaller than 0.
 Hence we propose following exponential update
\begin_inset Formula 
\[
\lambda_{k+1}=\lambda_{k}\left(1+\beta\mathbf{G}_{k}^{T}\widehat{\mathbf{G}}_{k-1}\right),
\]

\end_inset


\end_layout

\begin_layout Standard
and we constrain the update to following condition 
\begin_inset Formula $-1<\beta\mathbf{G}_{k}^{T}\widehat{\mathbf{G}}_{k-1}<1$
\end_inset

.
 Since 
\begin_inset Formula $\beta$
\end_inset

 is a hyperparameter we normalize 
\begin_inset Formula $\mathbf{G}/\left\Vert \mathbf{G}\right\Vert $
\end_inset

, where 
\begin_inset Formula $\left\Vert .\right\Vert $
\end_inset

 is a euclidean norm.
 Thus we have final update rule
\begin_inset Formula 
\[
\lambda_{k+1}=\lambda_{k}\left(1+\beta\frac{\mathbf{G}_{k}^{T}\mathbf{G}_{k-1}}{\left\Vert \mathbf{G}_{k}\right\Vert \left\Vert \mathbf{G}_{k-1}\right\Vert }\right),
\]

\end_inset

and 
\begin_inset Formula $\left|\beta\right|<1$
\end_inset

.
 Then we clip learning rate to certain range: 
\begin_inset Formula $\lambda_{k+1}=\mathrm{clip\left(\lambda_{k+1},\lambda_{\min},\lambda_{\max}\right)}$
\end_inset

, this helps to avoid explosion of lr.
\end_layout

\begin_layout Standard
Problems:
\end_layout

\begin_layout Itemize
We have introduced additional parameter.
\end_layout

\begin_layout Itemize
Learning rate may be too small.
 
\end_layout

\begin_layout Itemize
How to incorporate momentum into the equations above ??? 
\end_layout

\begin_layout Itemize
Can be show that obtained value for lr is somehow optimal? Some bound for
 it? 
\end_layout

\begin_layout Subsection
Assumptions
\end_layout

\begin_layout Standard
Direction of the gradients does not change significantly between batches,
 such that the sign of the correlation between two subsequent gradients
 
\begin_inset Formula $\mathbf{G}_{k}^{T}\widehat{\mathbf{G}}_{k-1}$
\end_inset

 is meaningful.
 
\end_layout

\begin_layout Standard
Optimal value of 
\begin_inset Formula $\lambda_{k}$
\end_inset

 changes slowly with 
\begin_inset Formula $k$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Convergence
\end_layout

\begin_layout Standard
Can we prove convergence of this method? 
\end_layout

\begin_layout Section
Final algorithm:
\end_layout

\begin_layout Itemize
Normalized gradient descent update:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{\boldsymbol{\theta}}_{k+1}=\boldsymbol{\theta}_{k}-\lambda_{k}\frac{\mathbf{G}_{k}}{\left\Vert \mathbf{G}_{k}\right\Vert _{\infty}+\varepsilon}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Learning rate update:
\begin_inset Formula 
\[
\lambda_{k+1}=\mathrm{clip}\left[\lambda_{k}\left(1+\beta\frac{\mathbf{G}_{k}^{T}\mathbf{G}_{k-1}}{\left\Vert \mathbf{G}_{k}\right\Vert \left\Vert \mathbf{G}_{k-1}\right\Vert }\right),\lambda_{\min},\lambda_{\max}\right],
\]

\end_inset


\end_layout

\begin_layout Itemize
Initial conditions: 
\begin_inset Formula $\mathbf{G}_{-1}=\mathbf{0}$
\end_inset

, and example setting for my test case 
\begin_inset Formula $\beta=0.01$
\end_inset

, 
\begin_inset Formula $\lambda_{0}=0.1$
\end_inset

, 
\begin_inset Formula $\lambda_{\min}=1e-5$
\end_inset

, 
\begin_inset Formula $\lambda_{\max}=0.1$
\end_inset


\end_layout

\end_body
\end_document
